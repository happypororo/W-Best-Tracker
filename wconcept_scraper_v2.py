#!/usr/bin/env python3
"""
Wì»¨ì…‰ ë² ìŠ¤íŠ¸ ìƒí’ˆ í¬ë¡¤ëŸ¬ v2
ì‹¤ì œ URL: https://display.wconcept.co.kr/rn/best?displayCategoryType=10101&displaySubCategoryType=10101201&gnbType=Y
"""

import asyncio
import json
from datetime import datetime, timezone
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import re

class WConceptScraper:
    """Wì»¨ì…‰ ë² ìŠ¤íŠ¸ ìƒí’ˆ í¬ë¡¤ëŸ¬"""
    
    # ì¹´í…Œê³ ë¦¬ ì •ì˜
    CATEGORIES = {
        'outer': {'name': 'ì•„ìš°í„°', 'sub_category': '10101201'},
        'dress': {'name': 'ì›í”¼ìŠ¤', 'sub_category': '10101202'},
        'blouse': {'name': 'ë¸”ë¼ìš°ìŠ¤', 'sub_category': '10101203'},
        'shirt': {'name': 'ì…”ì¸ ', 'sub_category': '10101204'},
        'tshirt': {'name': 'í‹°ì…”ì¸ ', 'sub_category': '10101205'},
        'knit': {'name': 'ë‹ˆíŠ¸', 'sub_category': '10101206'},
        'skirt': {'name': 'ìŠ¤ì»¤íŠ¸', 'sub_category': '10101207'},
        'underwear': {'name': 'ì–¸ë”ì›¨ì–´', 'sub_category': '10101212'},
    }
    
    def __init__(self, category_key='outer'):
        """
        Args:
            category_key: 'outer', 'dress', 'blouse', 'shirt', 'tshirt', 'knit', 'skirt', 'underwear' ì¤‘ í•˜ë‚˜
        """
        if category_key not in self.CATEGORIES:
            raise ValueError(f"Invalid category. Choose from: {list(self.CATEGORIES.keys())}")
        
        self.category_key = category_key
        self.category_name = self.CATEGORIES[category_key]['name']
        sub_category = self.CATEGORIES[category_key]['sub_category']
        self.url = f"https://display.wconcept.co.kr/rn/best?displayCategoryType=10101&displaySubCategoryType={sub_category}&gnbType=Y"
        self.products = []
    
    async def scrape(self, max_products=200):
        """ìƒí’ˆ ë°ì´í„° í¬ë¡¤ë§"""
        
        print("=" * 70)
        print(f"Wì»¨ì…‰ ë² ìŠ¤íŠ¸ ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘ - {self.category_name}")
        print("=" * 70)
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {self.category_name} ({self.category_key})")
        print(f"ğŸ¯ ëª©í‘œ: {max_products}ê°œ ìƒí’ˆ ìˆ˜ì§‘")
        print(f"ğŸ”— URL: {self.url}")
        print()
        
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            print("ğŸŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...")
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            
            page = await context.new_page()
            
            try:
                # í˜ì´ì§€ ì ‘ì†
                print(f"ğŸ“¡ í˜ì´ì§€ ì ‘ì† ì¤‘...")
                await page.goto(self.url, wait_until='domcontentloaded', timeout=30000)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                print("â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°...")
                await asyncio.sleep(5)
                
                # ìƒí’ˆì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    await page.wait_for_selector('div.product-item', timeout=10000)
                    print("âœ“ ìƒí’ˆ ìš”ì†Œ ë¡œë“œë¨")
                except:
                    print("âš ï¸  ìƒí’ˆ ìš”ì†Œ ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ê³„ì† ì§„í–‰)")
                
                # ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ìƒí’ˆ ë¡œë“œ
                print(f"ğŸ“œ ìŠ¤í¬ë¡¤í•˜ì—¬ {max_products}ê°œ ìƒí’ˆ ë¡œë”©...")
                await self._scroll_to_load_products(page, max_products)
                
                # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
                content = await page.content()
                
                # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                print("ğŸ” HTML íŒŒì‹± ì¤‘...")
                soup = BeautifulSoup(content, 'html.parser')
                
                # ìƒí’ˆ ìš”ì†Œ ì°¾ê¸°
                product_elements = soup.select('div.product-item')
                print(f"âœ… ë°œê²¬ëœ ìƒí’ˆ: {len(product_elements)}ê°œ")
                
                # ê° ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
                print("\nğŸ“¦ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì¤‘...")
                for idx, elem in enumerate(product_elements[:max_products], 1):
                    try:
                        product = self._extract_product_info(elem, idx)
                        if product:
                            self.products.append(product)
                            if idx % 20 == 0:
                                print(f"   ì§„í–‰: {idx}/{max_products}...")
                    except Exception as e:
                        print(f"   âš ï¸  ìƒí’ˆ {idx} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                
                print(f"\nâœ… ì´ {len(self.products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ê²°ê³¼ ì €ì¥
                self._save_results()
                
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                import traceback
                traceback.print_exc()
            finally:
                await browser.close()
                print("\nâœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ")
        
        return self.products
    
    async def _scroll_to_load_products(self, page, target_count):
        """ìŠ¤í¬ë¡¤í•˜ì—¬ ìƒí’ˆ ë¡œë“œ"""
        last_count = 0
        scroll_attempts = 0
        max_scrolls = 20  # ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜
        
        while scroll_attempts < max_scrolls:
            # í˜„ì¬ ë¡œë“œëœ ìƒí’ˆ ìˆ˜ í™•ì¸
            current_count = await page.locator('div.product-item').count()
            
            if current_count >= target_count:
                print(f"   âœ“ {current_count}ê°œ ìƒí’ˆ ë¡œë“œë¨")
                break
            
            # ë³€í™”ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            if current_count == last_count:
                scroll_attempts += 1
                if scroll_attempts >= 3:
                    print(f"   âš ï¸  ë” ì´ìƒ ë¡œë“œë˜ì§€ ì•ŠìŒ (í˜„ì¬: {current_count}ê°œ)")
                    break
            else:
                scroll_attempts = 0
            
            last_count = current_count
            
            # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await asyncio.sleep(1.5)
    
    def _extract_product_info(self, elem, rank):
        """ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        
        # ë¸Œëœë“œëª…ê³¼ ìƒí’ˆëª… (prdc-title ì•ˆì— ìˆìŒ)
        title_section = elem.select_one('.prdc-title')
        if title_section:
            title_spans = title_section.select('span.text')
            brand_name = title_spans[0].get_text(strip=True) if len(title_spans) > 0 else "N/A"
            product_name = title_spans[1].get_text(strip=True) if len(title_spans) > 1 else "N/A"
        else:
            brand_name = "N/A"
            product_name = "N/A"
        
        # ê°€ê²© ì •ë³´
        price_info = self._extract_price_info(elem)
        
        # ì´ë¯¸ì§€
        img_elem = elem.select_one('img')
        image_url = img_elem.get('src') or img_elem.get('data-src') if img_elem else "N/A"
        
        # ìƒí’ˆ ID ë° URL ì¶”ì¶œ
        product_id = None
        product_url = "N/A"
        
        # ì´ë¯¸ì§€ URLì—ì„œ ìƒí’ˆ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: 307602440_MA70111.jpg â†’ 307602440)
        if image_url and image_url != "N/A":
            img_match = re.search(r'/(\d+)_[A-Z0-9]+\.jpg', image_url)
            if img_match:
                product_num = img_match.group(1)
                product_id = f"PROD_{product_num}"
                # W Concept ì œí’ˆ ìƒì„¸ URL êµ¬ì„±
                product_url = f"https://www.wconcept.co.kr/Product/{product_num}"
        
        # ì´ë¯¸ì§€ì—ì„œ ì‹¤íŒ¨í•˜ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if not product_id:
            # a íƒœê·¸ì—ì„œ href ì°¾ê¸°
            link_elem = elem.select_one('a')
            if link_elem:
                href = link_elem.get('href', "N/A")
                if href and href != "N/A":
                    if not href.startswith('http'):
                        product_url = f"https://www.wconcept.co.kr{href}"
                    else:
                        product_url = href
                    # URLì—ì„œ ID ì¶”ì¶œ
                    product_id = self._extract_product_id(product_url)
        
        # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ rankì™€ ë¸Œëœë“œ/ìƒí’ˆëª… ì¡°í•©ìœ¼ë¡œ ê³ ìœ  ID ìƒì„±
        if not product_id or product_id == "PROD_0":
            unique_str = f"{rank}_{brand_name}_{product_name}"
            product_id = f"PROD_{abs(hash(unique_str)) % 10000000:07d}"
            # URLë„ ì—†ìœ¼ë©´ N/Aë¡œ ìœ ì§€
        
        return {
            'rank': rank,
            'product_id': product_id,
            'product_name': product_name,
            'brand_name': brand_name,
            'category': self.category_name,
            'category_key': self.category_key,
            'original_price': price_info['original_price'],
            'sale_price': price_info['sale_price'],
            'discount_rate': price_info['discount_rate'],
            'image_url': image_url,
            'product_url': product_url,
            'collected_at': datetime.now(timezone.utc).isoformat()
        }
    
    def _extract_price_info(self, elem):
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ"""
        price_info = {
            'original_price': None,
            'sale_price': None,
            'discount_rate': None
        }
        
        # ìˆ«ìë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        def extract_number(text):
            if not text:
                return None
            # ìˆ«ìì™€ ì‰¼í‘œë§Œ ì¶”ì¶œ
            numbers = re.findall(r'[\d,]+', text)
            if numbers:
                return int(numbers[0].replace(',', ''))
            return None
        
        # ê°€ê²© ì„¹ì…˜ ì°¾ê¸°
        price_section = elem.select_one('.prdc-price')
        if price_section:
            # ì›ê°€
            customer_price_elem = price_section.select_one('.customer-price')
            if customer_price_elem:
                price_info['original_price'] = extract_number(customer_price_elem.get_text(strip=True))
            
            # í• ì¸ìœ¨
            discount_elem = price_section.select_one('.final-discount em')
            if discount_elem:
                discount_match = re.search(r'(\d+)', discount_elem.get_text(strip=True))
                if discount_match:
                    price_info['discount_rate'] = int(discount_match.group(1))
            
            # ìµœì¢… íŒë§¤ê°€
            final_price_elem = price_section.select_one('.final-price strong')
            if final_price_elem:
                price_info['sale_price'] = extract_number(final_price_elem.get_text(strip=True))
        
        # í• ì¸ê°€ê°€ ì—†ìœ¼ë©´ íŒë§¤ê°€ë¥¼ ì›ê°€ë¡œ
        if not price_info['sale_price'] and price_info['original_price']:
            price_info['sale_price'] = price_info['original_price']
        
        return price_info
    
    def _extract_product_id(self, url):
        """URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ"""
        if not url or url == "N/A":
            # URLì´ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ URLì—ì„œ ID ì¶”ì¶œ ì‹œë„
            return None
        
        # URL íŒ¨í„´ì—ì„œ ID ì¶”ì¶œ ì‹œë„
        patterns = [
            r'/product/(\d+)',
            r'/goods/(\d+)',
            r'productId=(\d+)',
            r'goodsId=(\d+)',
            r'/(\d+)$',  # URL ëì˜ ìˆ«ì
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return f"PROD_{match.group(1)}"
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ URL í•´ì‹œê°’ ì‚¬ìš©
        return f"PROD_{abs(hash(url)) % 1000000:06d}"
    
    def _save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
        
        # JSON íŒŒì¼ë¡œ ì €ì¥ (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
        json_file = f'wconcept_data_{self.category_key}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'category': self.category_name,
                'category_key': self.category_key,
                'collected_at': datetime.now(timezone.utc).isoformat(),
                'total_products': len(self.products),
                'products': self.products
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥: {json_file}")
        
        # ìš”ì•½ í†µê³„
        self._print_summary()
    
    def _print_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½"""
        if not self.products:
            return
        
        print("\n" + "=" * 70)
        print("ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 70)
        
        # ë¸Œëœë“œë³„ ì§‘ê³„
        brand_counts = {}
        for product in self.products:
            brand = product['brand_name']
            brand_counts[brand] = brand_counts.get(brand, 0) + 1
        
        # ìƒìœ„ 10ê°œ ë¸Œëœë“œ
        top_brands = sorted(brand_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        print(f"\nğŸ“Š ìƒìœ„ 10ê°œ ë¸Œëœë“œ:")
        for i, (brand, count) in enumerate(top_brands, 1):
            print(f"   {i:2d}. {brand:30s} : {count:3d}ê°œ")
        
        # ê°€ê²© í†µê³„
        prices = [p['sale_price'] for p in self.products if p['sale_price']]
        if prices:
            print(f"\nğŸ’° ê°€ê²© í†µê³„:")
            print(f"   í‰ê· ê°€: {sum(prices) / len(prices):,.0f}ì›")
            print(f"   ìµœì €ê°€: {min(prices):,.0f}ì›")
            print(f"   ìµœê³ ê°€: {max(prices):,.0f}ì›")
        
        # í• ì¸ í†µê³„
        discounted = [p for p in self.products if p['discount_rate']]
        if discounted:
            discount_rates = [p['discount_rate'] for p in discounted]
            print(f"\nğŸ·ï¸  í• ì¸ í†µê³„:")
            print(f"   í• ì¸ ìƒí’ˆ: {len(discounted)}ê°œ ({len(discounted)/len(self.products)*100:.1f}%)")
            print(f"   í‰ê·  í• ì¸ìœ¨: {sum(discount_rates) / len(discount_rates):.1f}%")
            print(f"   ìµœëŒ€ í• ì¸ìœ¨: {max(discount_rates)}%")


async def scrape_all_categories(max_products=200):
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
    all_products = []
    
    print("\n" + "=" * 70)
    print("ğŸš€ ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 70)
    
    for category_key in WConceptScraper.CATEGORIES.keys():
        try:
            print(f"\n{'='*70}")
            scraper = WConceptScraper(category_key=category_key)
            products = await scraper.scrape(max_products=max_products)
            all_products.extend(products)
            print(f"âœ… {scraper.category_name}: {len(products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {category_key} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    print(f"\n{'='*70}")
    print(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")
    print(f"{'='*70}")
    
    return all_products


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìë¡œ ì¹´í…Œê³ ë¦¬ ì§€ì • ê°€ëŠ¥
    if len(sys.argv) > 1:
        category_key = sys.argv[1]
        if category_key == 'all':
            products = await scrape_all_categories(max_products=200)
        else:
            scraper = WConceptScraper(category_key=category_key)
            products = await scraper.scrape(max_products=200)
    else:
        # ê¸°ë³¸: ì•„ìš°í„° ì¹´í…Œê³ ë¦¬ë§Œ
        scraper = WConceptScraper(category_key='outer')
        products = await scraper.scrape(max_products=200)
    
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ë¨")
    
    # ìƒ˜í”Œ ì¶œë ¥
    if products:
        print("\nğŸ“¦ ìƒ˜í”Œ ìƒí’ˆ (ì²« 3ê°œ):")
        for product in products[:3]:
            print(f"\n   [{product['rank']}] {product['brand_name']} - {product['product_name'][:50]}")
            print(f"       ì¹´í…Œê³ ë¦¬: {product['category']}")
            print(f"       ê°€ê²©: {product['sale_price']:,}ì›")
            if product['discount_rate']:
                print(f"       í• ì¸: {product['discount_rate']}% OFF")

if __name__ == "__main__":
    asyncio.run(main())
