name: Scheduled Crawl and Deploy

on:
  schedule:
    # ë§¤ì‹œê°„ 16ë¶„ (UTC ê¸°ì¤€)
    # UTC 11:16 = KST 20:20
    # UTC 12:16 = KST 21:20
    - cron: '16 * * * *'
  workflow_dispatch:  # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

jobs:
  crawl-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ”„ Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: ğŸ“¦ Install dependencies
        run: |
          pip install playwright beautifulsoup4 requests
          playwright install chromium --with-deps
      
      - name: ğŸ•·ï¸ Run crawling
        run: |
          python3 auto_crawl.py
        env:
          PYTHONUNBUFFERED: '1'
      
      - name: ğŸ“Š Check DB file
        run: |
          ls -lh wconcept_tracking.db
          echo "DB file size: $(du -h wconcept_tracking.db | cut -f1)"
      
      - name: ğŸ’¾ Commit updated DB
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add wconcept_tracking.db
          
          # ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì»¤ë°‹
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            TIMESTAMP=$(TZ=Asia/Seoul date '+%Y-%m-%d %H:%M:%S KST')
            git commit -m "chore: Update crawl data - $TIMESTAMP"
            git push
            echo "âœ… DB updated and pushed"
          fi
      
      # Fly.ioëŠ” main ë¸Œëœì¹˜ pushë¡œ ìë™ ì¬ë°°í¬ë¨!
